---
title: "High School Drop Out Rate"
date: "2/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
```

```{r}
library(knitr)
library(tidyverse)
library(caret)
library(DMwR)
library(rpart)
library(ROCR)
library(randomForest)
library(xgboost)
library(rpart.plot)

#----------------------------------------------------------------
#' #1. Collect and Prepare the Data
#----------------------------------------------------------------

data_orig <- read_csv("case3data.csv")

data <- data_orig

data$grade <- as.factor(data$grade)
data$dropped <- as.factor(data$dropped)
data$ethnicity <- as.factor(data$ethnicity)
data$sex <- as.factor(data$sex)
data$zip<- as.factor(data$zip)
data$subsidizedLunches<- as.factor(data$subsidizedLunches)
data$sanctions <- as.factor(data$sanctions)
data$athleticSeasons <- as.factor(data$athleticSeasons)
data$year <- as.factor(data$year)
data$gpa <- round(data$gpa,3)


data %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value,fill=key), color="black") +
  facet_wrap(~ key, scales = "free") +
  theme_minimal() + 
  theme(legend.position = 'none')

data %>%
  keep(is.factor) %>%
  gather() %>%
  group_by(key,value) %>% 
  summarise(n = n()) %>% 
  ggplot() +
  geom_bar(mapping=aes(x = value, y = n, fill=key), color="black", stat='identity') + 
  coord_flip() +
  facet_wrap(~ key, scales = "free") +
  theme_minimal() +
  theme(legend.position = 'none')


# Partition the data using caret's createDataPartition() function.

sample <- data %>% 
  filter(year!=2017)
set.seed(1234)
data.train <- sample
data.train <- SMOTE(dropped ~ ., data.frame(sample), perc.over = 200, perc.under = 250)

data_test <- data %>% 
  filter(year ==2017)

data.train <- data.train %>% 
  select(-studentID,-year)

```

#logistic
```{r}

logit_mod <-
  glm(dropped ~ ., family = binomial(link = 'logit'), data = data.train)

#' View the results of the model.
summary(logit_mod)

###backward
logit_mod <- step(logit_mod,direction="backward",trace=FALSE)
summary(logit_mod)

# LOGISTIC REGRESSION
# Train the model.
data_test <- data_test %>% 
  select(-year,-studentID)
logit.pred.prob <- predict(logit_mod, data_test, type = 'response')

# Using a decision boundary of 0.5 (i.e If P(y=1|X) > 0.5 then y="Yes" else y="No").
logit.pred <- ifelse(logit.pred.prob > 0.5, "1", "0")

test <- data_test$dropped
pred <- logit.pred
prob <- logit.pred.prob

roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")


# Get performance metrics.
accuracy <- mean(test == pred)
precision <- posPredValue(as.factor(pred), as.factor(test), positive = "1")
recall <- sensitivity(as.factor(pred), as.factor(test), positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- confusionMatrix(as.factor(pred), test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
#kappa <- kappa2(data.frame(test, pred))$value
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- tibble(approach="Logistic Regression", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc)

```




```{r, fig.height=90,fig.width=80}
tree.mod <- train(dropped ~ ., data = data.train, method = "rpart")

tree.mod

tree.pred <- predict(tree.mod, data_test)

# View the Confusion Matrix.
confusionMatrix(tree.pred, data_test$dropped, positive = "1")

# Just like before, note that we can obtain predicted classes...
head(predict(tree.mod, data_test, type = "raw"))

# ...as well as the predicted probabilities (with "raw" and "prob", respectively).
head(predict(tree.mod, data_test, type = "prob"))


ctrl <-
  trainControl(method = "cv",
               number = 10,
               selectionFunction = "best")


grid <-
  expand.grid(
    .model = "tree",
    .trials = c(1, 5, 10, 15, 20, 25, 30, 35),
    .winnow = FALSE
  )



grid <- 
  expand.grid(
    .cp = seq(from=0.0001, to=0.005, by=0.0001)
)


set.seed(1234)
tree.mod <-
  train(
    dropped ~ .,
    data = data.train,
    method = "rpart",
    metric = "Kappa",
    trControl = ctrl,
    tuneGrid = grid
  )


library(rattle)
fancyRpartPlot(tree.mod$finalModel)

```


```{r}
#----------------------------------------------------------------
#' #4. Random Forest
#----------------------------------------------------------------


grid <- expand.grid(.mtry = c(3, 6, 9,12))



ctrl <-
  trainControl(method = "cv",
               number = 3,
               selectionFunction = "oneSE")


set.seed(1234)
rf.mod <-
  train(
    dropped ~ .,
    data = data.train,
    method = "rf",
    metric = "Kappa",
    trControl = ctrl,
    tuneGrid = grid
  )

rf.mod

```



```{r}
#----------------------------------------------------------------
#' #5. Extreme Gradient Boosting
#----------------------------------------------------------------

ctrl <-
  trainControl(method = "cv",
               number = 3,
               selectionFunction = "best")


grid <- expand.grid(
  nrounds = 20,
  max_depth = c(4, 6, 8),
  eta =  c(0.1, 0.3, 0.5),
  gamma = 0.01,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.5, 1)
)

set.seed(1234)
xgb.mod <-
  train(
    dropped ~ .,
    data = data.train,
    method = "xgbTree",
    metric = "Kappa",
    trControl = ctrl,
    tuneGrid = grid
  )

xgb.mod

```

```{r}


#----------------------------------------------------------------
#' #6. Compare Model Performance
#----------------------------------------------------------------

#' ##Logistic Regression
# Train the model.
logit.mod <-
  glm(dropped ~ ., family = binomial(link = 'logit'), data = data.train)

logit.pred.prob <- predict(logit.mod, data_test, type = 'response')

logit.pred <- as.factor(ifelse(logit.pred.prob > 0.5, "1", "0"))

test <- data_test$dropped
pred <- logit.pred
prob <- logit.pred.prob

# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")


# Get performance metrics.
accuracy <- mean(test == pred)
precision <- posPredValue(as.factor(pred), as.factor(test), positive = "1")
recall <- sensitivity(as.factor(pred), as.factor(test), positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- tibble(approach="Logistic Regression", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 



#' ##Classification Tree.
tree.pred <- predict(tree.mod, data_test, type = "raw")
tree.pred.prob <- predict(tree.mod, data_test, type = "prob")

test <- data_test$dropped
pred <- tree.pred
prob <- tree.pred.prob[,c("1")]

# Plot ROC Curve.
# dev.off()
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")



# Get performance metrics.
accuracy <- mean(test == pred)
precision <- posPredValue(as.factor(pred), as.factor(test), positive = "1")
recall <- sensitivity(as.factor(pred), as.factor(test), positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Decision Tree", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 



##randomforest
rf.pred <- predict(rf.mod, data_test, type = "raw")
rf.pred.prob <- predict(rf.mod, data_test, type = "prob")

test <- data_test$dropped
pred <- rf.pred
prob <- rf.pred.prob[,c("1")]


# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")


# Get performance metrics.
accuracy <- mean(test == pred)
precision <- posPredValue(as.factor(pred), as.factor(test), positive = "1")
recall <- sensitivity(as.factor(pred), as.factor(test), positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Random Forest", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 

#xgboost

xgb.pred <- predict(xgb.mod, data_test, type = "raw")
xgb.pred.prob <- predict(xgb.mod, data_test, type = "prob")

test <- data_test$dropped
pred <- xgb.pred
prob <- xgb.pred.prob[,c("1")]


# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")


# Get performance metrics.
accuracy <- mean(test == pred)
precision <- posPredValue(as.factor(pred), as.factor(test), positive = "1")
recall <- sensitivity(as.factor(pred), as.factor(test), positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Extreme Gradient Boosting", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 


#' ##Output Comparison Table.
kable(comparisons)



```


```{r}
# logistic
test <- data_test$dropped
pred <- logit.pred
prob <- logit.pred.prob

# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, main = "ROC Curve for Drop-out Rate Prediction Approaches",col=3, lwd = 2)+abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)

# Plot ROC Curve.
tree.pred <- predict(tree.mod, data_test, type = "raw")
tree.pred.prob <- predict(tree.mod, data_test, type = "prob")

test <- data_test$dropped
pred <- tree.pred
prob <- tree.pred.prob[,c("1")]

# Plot ROC Curve.
# dev.off()
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=2, lwd = 2, add=TRUE)

#' ##Random Forest.
rf.pred <- predict(rf.mod, data_test, type = "raw")
rf.pred.prob <- predict(rf.mod, data_test, type = "prob")

test <- data_test$dropped
pred <- rf.pred
prob <- rf.pred.prob[,c("1")]

# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=4, lwd = 2, add=TRUE)


#' ##XGBoost.
xgb.pred <- predict(xgb.mod, data_test, type = "raw")
xgb.pred.prob <- predict(xgb.mod, data_test, type = "prob")

test <- data_test$dropped
pred <- xgb.pred
prob <- xgb.pred.prob[,c("1")]

# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=5, lwd = 2, add=TRUE)


# Draw ROC legend.
legend(0.6, 0.6, c('Decision Tree','Logistic Regression',  'Random Forest', 'Extreme Gradient Boosting'), 2:5)

```

